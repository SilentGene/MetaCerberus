=====================================================
Start Time  : Mon Jul 10 01:51:40 EDT 2023
Submit Dir  : /projects/raw_lab/jobs/metacerberus
Job ID/Name : 4354831 / Metacerberus
Node List   : str-ac[1-2,4-5,8]
Num Tasks   : 5 total [5 nodes @ 64 CPUs/node]
======================================================

======================================================
Running Metacerberus On META/genomes/250mb.fna
======================================================
Initializing Ray on 5 Nodes
IP Head: 192.168.190.1:6379
Starting HEAD at str-ac1
2023-07-10 01:51:43,294	INFO usage_lib.py:408 -- Usage stats collection is enabled by default without user confirmation because this terminal is detected to be non-interactive. To disable this, add `--disable-usage-stats` to the command that starts the cluster, or run the following command: `ray disable-usage-stats` before starting the cluster. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.
2023-07-10 01:51:43,295	INFO scripts.py:712 -- [37mLocal node IP[39m: [1m192.168.190.1[22m
2023-07-10 01:51:45,374	SUCC scripts.py:749 -- [32m--------------------[39m
2023-07-10 01:51:45,374	SUCC scripts.py:750 -- [32mRay runtime started.[39m
2023-07-10 01:51:45,374	SUCC scripts.py:751 -- [32m--------------------[39m
2023-07-10 01:51:45,374	INFO scripts.py:753 -- [36mNext steps[39m
2023-07-10 01:51:45,374	INFO scripts.py:756 -- To add another node to this Ray cluster, run
2023-07-10 01:51:45,374	INFO scripts.py:759 -- [1m  ray start --address='192.168.190.1:6379'[22m
2023-07-10 01:51:45,374	INFO scripts.py:768 -- To connect to this Ray cluster:
2023-07-10 01:51:45,374	INFO scripts.py:770 -- [35mimport[39m[26m ray
2023-07-10 01:51:45,374	INFO scripts.py:771 -- ray[35m.[39m[26minit(_node_ip_address[35m=[39m[26m[33m'192.168.190.1'[39m[26m)
2023-07-10 01:51:45,374	INFO scripts.py:783 -- To submit a Ray job using the Ray Jobs CLI:
2023-07-10 01:51:45,374	INFO scripts.py:784 -- [1m  RAY_ADDRESS='http://127.0.0.1:8265' ray job submit --working-dir . -- python my_script.py[22m
2023-07-10 01:51:45,374	INFO scripts.py:793 -- See https://docs.ray.io/en/latest/cluster/running-applications/job-submission/index.html 
2023-07-10 01:51:45,374	INFO scripts.py:797 -- for more information on submitting Ray jobs to the Ray cluster.
2023-07-10 01:51:45,374	INFO scripts.py:802 -- To terminate the Ray runtime, run
2023-07-10 01:51:45,374	INFO scripts.py:803 -- [1m  ray stop[22m
2023-07-10 01:51:45,375	INFO scripts.py:806 -- To view the status of the cluster, use
2023-07-10 01:51:45,375	INFO scripts.py:807 --   [1mray status[22m[26m
2023-07-10 01:51:45,375	INFO scripts.py:811 -- To monitor and debug Ray, view the dashboard at 
2023-07-10 01:51:45,375	INFO scripts.py:812 --   [1m127.0.0.1:8265[22m[26m
2023-07-10 01:51:45,375	INFO scripts.py:819 -- [4mIf connection to the dashboard fails, check your firewall settings and network configuration.[24m
2023-07-10 01:51:45,375	INFO scripts.py:919 -- [36m[1m--block[22m[39m
2023-07-10 01:51:45,375	INFO scripts.py:920 -- This command will now block forever until terminated by a signal.
2023-07-10 01:51:45,375	INFO scripts.py:923 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.
Starting WORKER 1 at str-ac2
[2023-07-10 01:51:48,344 I 365942 365942] global_state_accessor.cc:356: This node has an IP address of 192.168.190.2, but we cannot find a local Raylet with the same address. This can happen when you connect to the Ray cluster with a different IP address or when connecting to a container.
2023-07-10 01:51:48,299	INFO scripts.py:894 -- [37mLocal node IP[39m: [1m192.168.190.2[22m
2023-07-10 01:51:48,344	SUCC scripts.py:906 -- [32m--------------------[39m
2023-07-10 01:51:48,344	SUCC scripts.py:907 -- [32mRay runtime started.[39m
2023-07-10 01:51:48,345	SUCC scripts.py:908 -- [32m--------------------[39m
2023-07-10 01:51:48,345	INFO scripts.py:910 -- To terminate the Ray runtime, run
2023-07-10 01:51:48,345	INFO scripts.py:911 -- [1m  ray stop[22m
2023-07-10 01:51:48,345	INFO utils.py:116 -- Overwriting previous Ray address (192.168.190.2:6379). Running ray.init() on this node will now connect to the new instance at 192.168.190.1:6379. To override this behavior, pass address=192.168.190.2:6379 to ray.init().
2023-07-10 01:51:48,345	INFO scripts.py:919 -- [36m[1m--block[22m[39m
2023-07-10 01:51:48,345	INFO scripts.py:920 -- This command will now block forever until terminated by a signal.
2023-07-10 01:51:48,345	INFO scripts.py:923 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.
Starting WORKER 2 at str-ac4
[2023-07-10 01:51:53,356 I 2630799 2630799] global_state_accessor.cc:356: This node has an IP address of 192.168.190.4, but we cannot find a local Raylet with the same address. This can happen when you connect to the Ray cluster with a different IP address or when connecting to a container.
2023-07-10 01:51:53,312	INFO scripts.py:894 -- [37mLocal node IP[39m: [1m192.168.190.4[22m
2023-07-10 01:51:53,357	SUCC scripts.py:906 -- [32m--------------------[39m
2023-07-10 01:51:53,357	SUCC scripts.py:907 -- [32mRay runtime started.[39m
2023-07-10 01:51:53,357	SUCC scripts.py:908 -- [32m--------------------[39m
2023-07-10 01:51:53,357	INFO scripts.py:910 -- To terminate the Ray runtime, run
2023-07-10 01:51:53,357	INFO scripts.py:911 -- [1m  ray stop[22m
2023-07-10 01:51:53,357	INFO utils.py:116 -- Overwriting previous Ray address (192.168.190.2:6379). Running ray.init() on this node will now connect to the new instance at 192.168.190.1:6379. To override this behavior, pass address=192.168.190.2:6379 to ray.init().
2023-07-10 01:51:53,358	INFO scripts.py:919 -- [36m[1m--block[22m[39m
2023-07-10 01:51:53,358	INFO scripts.py:920 -- This command will now block forever until terminated by a signal.
2023-07-10 01:51:53,358	INFO scripts.py:923 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.
Starting WORKER 3 at str-ac5
[2023-07-10 01:51:58,480 I 2219184 2219184] global_state_accessor.cc:356: This node has an IP address of 192.168.190.5, but we cannot find a local Raylet with the same address. This can happen when you connect to the Ray cluster with a different IP address or when connecting to a container.
2023-07-10 01:51:58,432	INFO scripts.py:894 -- [37mLocal node IP[39m: [1m192.168.190.5[22m
2023-07-10 01:51:58,480	SUCC scripts.py:906 -- [32m--------------------[39m
2023-07-10 01:51:58,480	SUCC scripts.py:907 -- [32mRay runtime started.[39m
2023-07-10 01:51:58,480	SUCC scripts.py:908 -- [32m--------------------[39m
2023-07-10 01:51:58,480	INFO scripts.py:910 -- To terminate the Ray runtime, run
2023-07-10 01:51:58,480	INFO scripts.py:911 -- [1m  ray stop[22m
2023-07-10 01:51:58,480	INFO utils.py:116 -- Overwriting previous Ray address (192.168.190.2:6379). Running ray.init() on this node will now connect to the new instance at 192.168.190.1:6379. To override this behavior, pass address=192.168.190.2:6379 to ray.init().
2023-07-10 01:51:58,481	INFO scripts.py:919 -- [36m[1m--block[22m[39m
2023-07-10 01:51:58,481	INFO scripts.py:920 -- This command will now block forever until terminated by a signal.
2023-07-10 01:51:58,481	INFO scripts.py:923 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.
Starting WORKER 4 at str-ac8
[2023-07-10 01:52:03,419 I 2699432 2699432] global_state_accessor.cc:356: This node has an IP address of 192.168.190.8, but we cannot find a local Raylet with the same address. This can happen when you connect to the Ray cluster with a different IP address or when connecting to a container.
2023-07-10 01:52:03,372	INFO scripts.py:894 -- [37mLocal node IP[39m: [1m192.168.190.8[22m
2023-07-10 01:52:03,420	SUCC scripts.py:906 -- [32m--------------------[39m
2023-07-10 01:52:03,420	SUCC scripts.py:907 -- [32mRay runtime started.[39m
2023-07-10 01:52:03,420	SUCC scripts.py:908 -- [32m--------------------[39m
2023-07-10 01:52:03,420	INFO scripts.py:910 -- To terminate the Ray runtime, run
2023-07-10 01:52:03,420	INFO scripts.py:911 -- [1m  ray stop[22m
2023-07-10 01:52:03,420	INFO utils.py:116 -- Overwriting previous Ray address (192.168.190.2:6379). Running ray.init() on this node will now connect to the new instance at 192.168.190.1:6379. To override this behavior, pass address=192.168.190.2:6379 to ray.init().
2023-07-10 01:52:03,420	INFO scripts.py:919 -- [36m[1m--block[22m[39m
2023-07-10 01:52:03,421	INFO scripts.py:920 -- This command will now block forever until terminated by a signal.
2023-07-10 01:52:03,421	INFO scripts.py:923 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.
HMM: 'KOFam_all'
HMM: 'COG'
HMM: 'VOG'
HMM: 'PHROG'
HMM: 'CAZy'

Starting MetaCerberus Pipeline

Checking for external dependencies:
fastqc               /users/jlfiguer/.conda/envs/metacerberus-dev/bin/fastqc
flash2               /users/jlfiguer/.conda/envs/metacerberus-dev/bin/flash2
fastp                /users/jlfiguer/.conda/envs/metacerberus-dev/bin/fastp
porechop             /users/jlfiguer/.conda/envs/metacerberus-dev/bin/porechop
bbduk.sh             /users/jlfiguer/.conda/envs/metacerberus-dev/bin/bbduk.sh
FragGeneScanRs       /users/jlfiguer/.conda/envs/metacerberus-dev/lib/python3.10/site-packages/meta_cerberus/FGS/FragGeneScanRs
prodigal             /users/jlfiguer/.conda/envs/metacerberus-dev/bin/prodigal
hmmsearch            /users/jlfiguer/.conda/envs/metacerberus-dev/bin/hmmsearch
countAssembly.py     /users/jlfiguer/.conda/envs/metacerberus-dev/bin/countAssembly.py
Initializing RAY
2023-07-10 01:52:09,282	INFO worker.py:1452 -- Connecting to existing Ray cluster at address: 192.168.190.1:6379...
2023-07-10 01:52:09,353	INFO worker.py:1627 -- Connected to Ray cluster. View the dashboard at [1m[32m127.0.0.1:8265 [39m[22m
Started RAY on cluster
Running RAY on 5 node(s)
Using 64 CPUs per node

STEP 1: Loading sequence files:
Processing 0 fastq sequences
Processing 1 fasta sequences
Processing 0 protein Sequences

STEP 5a: Removing N's from contig files

STEP 6: Metaome Stats


STEP 7: ORF Finder


STEP 8: HMMER Search


STEP 10: Creating Reports
Saving Statistics
Creating Rollup Tables
Creating Count Tables
NOTE: PCA Tables created only when there are at least four sequence files.

NOTE: Pathview created only when there are at least four sequence files.

Creating combined sunburst and bargraphs

Finished Pipeline
22.76user 9.03system 6:46.31elapsed 7%CPU (0avgtext+0avgdata 221596maxresident)k
536016inputs+4541856outputs (642major+89350minor)pagefaults 0swaps

======================================================
End Time   : Mon Jul 10 01:58:53 EDT 2023
Run Time   : 433 seconds
======================================================

Disk Used 540508 results/Metacerberus/META/genomes/250mb
